# ğŸ¤– OllamaBot

<div align="center">

![OllamaBot Banner](https://img.shields.io/badge/OllamaBot-Local_AI_IDE-7dcfff?style=for-the-badge&logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9IiM3ZGNmZmYiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIj48cGF0aCBkPSJNMTggOGE2IDYgMCAwIDAtMTIgMGMwIDcgMTIgNyAxMiAwWiIvPjxjaXJjbGUgY3g9IjEyIiBjeT0iOCIgcj0iNiIvPjwvc3ZnPg==)

**A native macOS IDE with Infinite Mode â€” autonomous AI agents powered by local Ollama models**

[![macOS](https://img.shields.io/badge/macOS-14.0+-000000?style=flat-square&logo=apple&logoColor=white)](https://www.apple.com/macos/)
[![Swift](https://img.shields.io/badge/Swift-5.9+-F05138?style=flat-square&logo=swift&logoColor=white)](https://swift.org)
[![Ollama](https://img.shields.io/badge/Ollama-Local_AI-white?style=flat-square)](https://ollama.ai)
[![License](https://img.shields.io/badge/License-MIT-9ece6a?style=flat-square)](LICENSE)

[Features](#-features) â€¢ [Installation](#-installation) â€¢ [Usage](#-usage) â€¢ [Architecture](#-architecture) â€¢ [Configuration](#-configuration)

</div>

---

## âœ¨ What Makes OllamaBot Different

Traditional AI coding tools wait for your commands. **OllamaBot's Infinite Mode** flips this paradigm:

- ğŸ”„ **Autonomous Operation** â€” Give it a task, watch it work until completion
- ğŸ§  **Multi-Model Orchestration** â€” 4 specialized AI models working in coordination  
- ğŸ’» **100% Local** â€” No API costs, no usage limits, complete privacy
- âš¡ **Apple Silicon Optimized** â€” Built for M1/M2/M3 performance

---

## ğŸ­ The Model Orchestra

OllamaBot coordinates four specialized 32B parameter models, each excelling at different tasks:

| Model | Role | Color | Specialization |
|-------|------|-------|----------------|
| **Qwen3 32B** | ğŸ§  Orchestrator | ![#bb9af7](https://via.placeholder.com/12/bb9af7/bb9af7.png) Purple | Thinking, planning, delegating tasks |
| **Command-R 35B** | ğŸ” Researcher | ![#7aa2f7](https://via.placeholder.com/12/7aa2f7/7aa2f7.png) Blue | Research, RAG, documentation |
| **Qwen2.5-Coder 32B** | ğŸ’» Coder | ![#ff9e64](https://via.placeholder.com/12/ff9e64/ff9e64.png) Orange | Code generation, debugging, refactoring |
| **Qwen3-VL 32B** | ğŸ‘ï¸ Vision | ![#9ece6a](https://via.placeholder.com/12/9ece6a/9ece6a.png) Green | Image analysis, UI inspection |

---

## ğŸš€ Features

### ğŸ”® Infinite Mode (The Star Feature)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     YOU GIVE A TASK                         â”‚
â”‚           "Add user authentication to this app"             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              QWEN3 (ORCHESTRATOR) - The Brain               â”‚
â”‚                                                             â”‚
â”‚   Uses tools: think, read_file, search_files, list_dir,    â”‚
â”‚   delegate_to_coder, delegate_to_researcher, etc.          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“                    â†“                    â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ COMMAND-R    â”‚   â”‚ QWEN-CODER   â”‚   â”‚ QWEN3-VL     â”‚
    â”‚ (Research)   â”‚   â”‚ (Coding)     â”‚   â”‚ (Vision)     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
              [Results fed back to Orchestrator]
                              â†“
                [Loop continues until complete]
```

**18 Built-in Agent Tools:**

| Category | Tool | Description |
|----------|------|-------------|
| **Core** | `think` | Plan and reason about the task |
| | `complete` | Signal task completion |
| | `ask_user` | Request user input |
| **Files** | `read_file` | Read file contents |
| | `write_file` | Create or overwrite files |
| | `edit_file` | Search and replace in files |
| | `search_files` | Search text across the codebase |
| | `list_directory` | Explore directory structure |
| **System** | `run_command` | Execute shell commands |
| | `take_screenshot` | Capture screen for vision analysis |
| **AI Delegation** | `delegate_to_coder` | Send coding tasks to Qwen-Coder |
| | `delegate_to_researcher` | Send research tasks to Command-R |
| | `delegate_to_vision` | Send image analysis to Qwen-VL |
| **Web** | `web_search` | Search the web via DuckDuckGo |
| | `fetch_url` | Fetch and extract web page content |
| **Git** | `git_status` | Get repository status |
| | `git_diff` | View file or repo diffs |
| | `git_commit` | Stage and commit changes |

### ğŸ’¬ Chat Mode

- Quick conversations with any model
- **Auto-routing** based on question type
- Manual model override with keyboard shortcuts
- Context-aware â€” includes open files and selections
- `@filename` mentions for additional context
- **Persistent chat history** â€” conversations saved across sessions

### ğŸ“Š Competitive Benchmark

| Feature | Cursor | Windsurf | VS Code | **OllamaBot** |
|---------|:------:|:--------:|:-------:|:-------------:|
| Inline Tab Completions | âœ… | âœ… | âœ… (Copilot) | âœ… |
| Chat with AI | âœ… | âœ… | âœ… | âœ… |
| Agentic Mode | âœ… | âœ… (Cascade) | âŒ | âœ… (Infinite) |
| **Multi-Model Orchestration** | âŒ | âŒ | âŒ | **âœ…** |
| @ Mentions | âœ… | âœ… | âœ… | âœ… |
| Diff View | âœ… | âœ… | âœ… | âœ… |
| Git Integration | âœ… | âœ… | âœ… | âœ… |
| Web Search | âœ… | âœ… | âŒ | âœ… |
| Chat History | âœ… | âœ… | âœ… | âœ… |
| Symbol Outline | âœ… | âœ… | âœ… | âœ… |
| Problems Panel | âœ… | âœ… | âœ… | âœ… |
| **100% Local/Private** | âŒ | âŒ | âŒ | **âœ…** |
| **No API Costs** | âŒ | âŒ | âŒ | **âœ…** |
| Native macOS | âŒ | âŒ | âŒ | **âœ…** |

### ğŸ–¥ï¸ Full IDE

- **File Explorer** with syntax-colored icons
- **Code Editor** with line numbers, syntax highlighting
- **Integrated Terminal** with PTY support
- **Multiple Tabs** with modification indicators
- **Breadcrumb Navigation**
- **Status Bar** with model/connection info
- **Command Palette** (`âŒ˜â‡§P`)
- **Quick Open** (`âŒ˜P`)
- **Global Search** (`âŒ˜â‡§F`)
- **Find & Replace** (`âŒ˜F` / `âŒ˜âŒ¥F`)
- **Go to Line** (`âŒƒG`)

### âš¡ Performance Optimized

| Layer | Optimization |
|-------|-------------|
| **File I/O** | Memory-mapped for files >64KB |
| **Caching** | LRU with `os_unfair_lock` |
| **Search** | Parallel trigram + word indexing |
| **Ollama** | Task-specific temperature/tokens |
| **Memory** | Auto-clear under pressure |
| **Models** | Pre-warmed on launch |

---

## ğŸ“¦ Installation

### Prerequisites

- **macOS 14.0** (Sonoma) or later
- **Apple Silicon** Mac (M1/M2/M3)
- **32GB RAM** minimum (for 32B models)
- **Ollama** installed

### Step 1: Install Ollama

```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

### Step 2: Pull the Models

```bash
# Orchestrator (required)
ollama pull qwen3:32b

# Research Model
ollama pull command-r:35b

# Coding Model
ollama pull qwen2.5-coder:32b

# Vision Model (optional)
ollama pull qwen3-vl:32b
```

### Step 3: Clone & Build

```bash
git clone https://github.com/cadenroberts/ollamabot.git
cd ollamabot

# Generate app icon (requires ImageMagick: brew install imagemagick)
./scripts/generate-icon.sh

# Build the app bundle
./scripts/build-app.sh --release
```

### Step 4: Install & Run

```bash
# Install to Applications
cp -r build/OllamaBot.app /Applications/

# Or run directly
open build/OllamaBot.app
```

Or for development:
```bash
swift run OllamaBot
```

Or open in Xcode:
```bash
open Package.swift
```

---

## ğŸ¯ Usage

### Infinite Mode

1. Press `âŒ˜â‡§I` or click the **âˆ** button
2. Describe your task:
   - *"Add dark mode support to all views"*
   - *"Refactor this codebase to use async/await"*
   - *"Create unit tests for the user service"*
   - *"Document all public functions"*
3. Click **Start** and watch it work
4. **Stop** anytime to take control

### Chat Mode

Type in the chat panel on the right. The model auto-selects based on your question, or force a specific model:

| Shortcut | Model |
|----------|-------|
| `âŒ˜â‡§1` | Qwen3 (Writing) |
| `âŒ˜â‡§2` | Command-R (Research) |
| `âŒ˜â‡§3` | Qwen-Coder (Coding) |
| `âŒ˜â‡§4` | Qwen-VL (Vision) |
| `âŒ˜â‡§0` | Auto-route |

### Keyboard Shortcuts

| Shortcut | Action |
|----------|--------|
| `âŒ˜O` | Open folder |
| `âŒ˜N` | New file |
| `âŒ˜S` | Save file |
| `âŒ˜P` | Quick open |
| `âŒ˜â‡§P` | Command palette |
| `âŒ˜F` | Find in file |
| `âŒ˜â‡§F` | Search in files |
| `âŒ˜âŒ¥F` | Find and replace |
| `âŒƒG` | Go to line |
| `âŒ˜B` | Toggle sidebar |
| `âŒƒ\`` | Toggle terminal |
| `âŒ˜â‡§I` | Toggle Infinite Mode |

---

## ğŸ—ï¸ Architecture

### Project Structure

```
OllamaBot/
â”œâ”€â”€ Sources/
â”‚   â”œâ”€â”€ OllamaBotApp.swift           # App entry, state management
â”‚   â”œâ”€â”€ Agent/
â”‚   â”‚   â”œâ”€â”€ AgentExecutor.swift      # Infinite Mode engine
â”‚   â”‚   â””â”€â”€ AgentTools.swift         # 18 tool definitions
â”‚   â”œâ”€â”€ Models/
â”‚   â”‚   â”œâ”€â”€ ChatMessage.swift        # Chat data model (Codable)
â”‚   â”‚   â”œâ”€â”€ FileItem.swift           # File tree model
â”‚   â”‚   â””â”€â”€ OllamaModel.swift        # Model enum + metadata
â”‚   â”œâ”€â”€ Services/
â”‚   â”‚   â”œâ”€â”€ OllamaService.swift      # Ollama API client
â”‚   â”‚   â”œâ”€â”€ ContextManager.swift     # ğŸ†• Comprehensive context management
â”‚   â”‚   â”œâ”€â”€ IntentRouter.swift       # Model routing logic
â”‚   â”‚   â”œâ”€â”€ ContextBuilder.swift     # Prompt construction
â”‚   â”‚   â”œâ”€â”€ FileIndexer.swift        # Background search index
â”‚   â”‚   â”œâ”€â”€ FileSystemService.swift  # File operations
â”‚   â”‚   â”œâ”€â”€ ConfigurationService.swift
â”‚   â”‚   â”œâ”€â”€ InlineCompletionService.swift  # Tab completions
â”‚   â”‚   â”œâ”€â”€ GitService.swift         # Git integration
â”‚   â”‚   â”œâ”€â”€ WebSearchService.swift   # DuckDuckGo search
â”‚   â”‚   â””â”€â”€ ChatHistoryService.swift # Persistent chat history
â”‚   â”œâ”€â”€ Utilities/
â”‚   â”‚   â”œâ”€â”€ DesignSystem.swift       # UI components & tokens
â”‚   â”‚   â”œâ”€â”€ PerformanceCore.swift    # Caches, async I/O
â”‚   â”‚   â”œâ”€â”€ StreamingBuffer.swift    # ğŸ†• Frame-rate limited UI updates
â”‚   â”‚   â”œâ”€â”€ SyntaxHighlighter.swift  # Code highlighting
â”‚   â”‚   â””â”€â”€ Benchmarks.swift         # Performance testing
â”‚   â””â”€â”€ Views/
â”‚       â”œâ”€â”€ MainView.swift           # Main layout
â”‚       â”œâ”€â”€ AgentView.swift          # Infinite Mode UI
â”‚       â”œâ”€â”€ ChatView.swift           # Chat panel (optimized MessageRow)
â”‚       â”œâ”€â”€ EditorView.swift         # Code editor
â”‚       â”œâ”€â”€ TerminalView.swift       # Terminal emulator
â”‚       â”œâ”€â”€ OutlineView.swift        # Symbol navigation
â”‚       â”œâ”€â”€ ProblemsPanel.swift      # Errors/warnings
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ Resources/
â”‚   â”œâ”€â”€ Info.plist                   # App bundle metadata
â”‚   â”œâ”€â”€ AppIcon.icns                 # App icon
â”‚   â””â”€â”€ icon.svg                     # Source icon
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ build-app.sh                 # Build .app bundle
â”‚   â””â”€â”€ generate-icon.sh             # Generate .icns from SVG
â”‚
â”œâ”€â”€ Package.swift                    # Swift Package Manager
â”œâ”€â”€ push.sh                          # Git push script
â””â”€â”€ README.md
```

### ğŸ§  Context Management System

OllamaBot's context management is the core differentiator from other AI IDEs. Here's how it works:

#### The Problem It Solves

AI models have limited context windows (8K-32K tokens). OllamaBot must intelligently:
1. **Prioritize** what context to include (selected code > open files > project structure)
2. **Compress** large contexts without losing critical information
3. **Pass context** between the orchestrator and specialist models
4. **Remember** past interactions and learn from errors

#### ContextManager Architecture

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚           ContextManager            â”‚
                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                    â”‚  â€¢ Token Budget Allocation          â”‚
                    â”‚  â€¢ Semantic Compression             â”‚
                    â”‚  â€¢ Inter-Agent Context Passing      â”‚
                    â”‚  â€¢ Conversation Memory              â”‚
                    â”‚  â€¢ Error Pattern Learning           â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   OrchestratorContext    â”‚ â”‚    DelegationContext       â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚  â€¢ Task description      â”‚ â”‚  â€¢ Optimized for specialistâ”‚
        â”‚  â€¢ Project structure     â”‚ â”‚  â€¢ Relevant files included â”‚
        â”‚  â€¢ Recent steps summary  â”‚ â”‚  â€¢ Context compressed      â”‚
        â”‚  â€¢ Relevant memories     â”‚ â”‚  â€¢ Model-specific prompts  â”‚
        â”‚  â€¢ Error warnings        â”‚ â”‚                            â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Token Budget Allocation

Each context section has a priority-based budget:

| Section | Priority | Max % of Budget |
|---------|----------|-----------------|
| **Task** | Critical | 25% |
| **File Content** | High | 33% |
| **Project** | High | 16% |
| **History** | Medium | 12% |
| **Memory** | Medium | 12% |
| **Errors** | High | 6% |

#### Inter-Agent Context Flow

```
User Task: "Fix the authentication bug"
                    â”‚
                    â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   ORCHESTRATOR (Qwen3)        â”‚
    â”‚                               â”‚
    â”‚  ContextManager builds:       â”‚
    â”‚  â€¢ Full task + project map    â”‚
    â”‚  â€¢ Past relevant memories     â”‚
    â”‚  â€¢ Error warnings             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â”‚ delegate_to_coder(task="Fix login validation")
                    â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   CODER (Qwen2.5-Coder)       â”‚
    â”‚                               â”‚
    â”‚  ContextManager builds:       â”‚
    â”‚  â€¢ Task (compressed)          â”‚
    â”‚  â€¢ Relevant files (extracted) â”‚
    â”‚  â€¢ Specialist system prompt   â”‚
    â”‚  â€¢ NO orchestrator bloat      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â”‚ Returns: Fixed code
                    â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   ORCHESTRATOR (Qwen3)        â”‚
    â”‚                               â”‚
    â”‚  Records:                     â”‚
    â”‚  â€¢ Tool result for reference  â”‚
    â”‚  â€¢ Memory entry for future    â”‚
    â”‚  â€¢ Verifies output validity   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Memory & Learning

The ContextManager maintains:

1. **Conversation Memory** - Past task/result pairs with relevance scoring
2. **Tool Results Buffer** - Recent 50 tool executions for reference
3. **Error Patterns** - Tracks recurring errors to warn the orchestrator

```swift
// Example: If "permissions" errors occur 2+ times, future tasks get warned:
"âš ï¸ WATCH OUT: Previously encountered issues with 'permissions'. Be careful."
```

### âš¡ Streaming Performance

#### The Problem

Naive implementation updates `@Observable` state on every token (~60/sec):
```swift
// âŒ BAD: 60 state mutations/sec = 60 SwiftUI diffs/sec = choppy UI
for try await chunk in stream {
    chatMessages[index].content += chunk
}
```

#### The Solution: Frame-Coalesced Updates

```swift
// âœ… GOOD: Batch updates to 30fps (every 33ms)
var buffer = ""
var lastUpdate = CACurrentMediaTime()

for try await chunk in stream {
    buffer.append(chunk)
    
    if CACurrentMediaTime() - lastUpdate >= 0.033 {  // 30fps
        chatMessages[index].content = buffer  // Single diff
        lastUpdate = CACurrentMediaTime()
    }
}
```

#### Additional Optimizations

| Component | Optimization |
|-----------|-------------|
| **MessageRow** | `Equatable` conformance - only re-renders when content changes |
| **AssistantContentView** | Cached markdown parsing - reparse only on content change |
| **OllamaService** | Buffer tokens to ~50 chars before yielding |
| **Throttler** | Rate-limits scroll, resize, search events |
| **Debouncer** | Delays expensive operations (search, highlight) |

### ğŸ”„ Model Routing (IntentRouter)

The IntentRouter automatically selects the best model based on the user's question:

```swift
// Keyword-based classification
"How do I implement async/await?" â†’ Coder (detected: "implement", "async", "await")
"What is quantum computing?"     â†’ Researcher (detected: "what is")
"Write me a haiku about coding"  â†’ Writing (detected: "write")
[Image attached]                 â†’ Vision (automatic)
```

Priority order:
1. **Vision** - If images attached
2. **Coder** - Code keywords + code context open
3. **Researcher** - Question words, "explain", "compare"
4. **Writing** - Default for general tasks

### ğŸ› ï¸ Tool Execution Pipeline

Agent tools execute in parallel when possible:

```
Tool Calls: [read_file(A), read_file(B), search_files(C)]
                           â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â–¼               â–¼               â–¼
    [Read File A]   [Read File B]   [Search Files]
           â”‚               â”‚               â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â–¼
              [Results aggregated & returned]
```

Non-parallelizable tools (write_file, run_command) execute sequentially.

---

## âš™ï¸ Configuration

Access settings via `âŒ˜,` or the menu bar.

### Editor
- Font family & size
- Tab size & spaces
- Word wrap
- Line numbers
- Minimap
- Auto-close brackets
- Format on save

### AI
- Default model
- Temperature (0.0 - 1.0)
- Max tokens
- Context window size
- Include file context
- Stream responses

### Agent (Infinite Mode)
- Max steps limit
- Allow terminal commands
- Allow file writes
- Confirm destructive actions

### Appearance
- Theme (System/Light/Dark)
- Sidebar width
- Status bar visibility
- Breadcrumbs

---

## ğŸ¨ Design System

OllamaBot uses a **Tokyo Night**-inspired color palette:

```swift
// Core Colors
background:     #1a1b26  // Deep background
surface:        #1f2335  // Cards, panels
accent:         #7dcfff  // Brand cyan

// Model Colors  
orchestrator:   #bb9af7  // Purple (Qwen3)
researcher:     #7aa2f7  // Blue (Command-R)
coder:          #ff9e64  // Orange (Qwen-Coder)
vision:         #9ece6a  // Green (Qwen-VL)

// Semantic
success:        #9ece6a
warning:        #e0af68
error:          #f7768e
```

---

## ğŸ”§ Troubleshooting

### "Ollama Disconnected"
```bash
ollama serve
```

### Slow Model Switching
Normal â€” 32B models take ~30s to load. The orchestrator stays warm.

### High Memory Usage
Use **Debug â†’ Clear Caches** or restart the app.

### Agent Seems Stuck
Check the step list â€” it may be thinking or waiting. Stop and retry with a more specific task.

---

## ğŸ“Š Performance

Run benchmarks: **Debug â†’ Run Performance Benchmarks**

Typical results on M1 Max 32GB:

| Metric | Value |
|--------|-------|
| Cache ops | ~400,000/sec |
| File reads (small) | ~35,000/sec |
| File reads (1MB) | ~20/sec (mmap) |
| Parallel speedup | 3.5x |
| First AI response | ~instant (warmed) |

---

## ğŸ¤ Contributing

Contributions welcome! This is an experiment in local AI autonomy.

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run `swift build` to verify
5. Submit a pull request

---

## ğŸ“„ License

MIT License â€” see [LICENSE](LICENSE) for details.

---

## ğŸ™ Acknowledgments

- [Ollama](https://ollama.ai) for making local LLMs accessible
- [SwiftTerm](https://github.com/migueldeicaza/SwiftTerm) for terminal emulation
- [Tokyo Night](https://github.com/enkia/tokyo-night-vscode-theme) for color inspiration

---

<div align="center">

**Built with â¤ï¸ for local AI enthusiasts**

*Your AI should work FOR you, not wait ON you.*

</div>
